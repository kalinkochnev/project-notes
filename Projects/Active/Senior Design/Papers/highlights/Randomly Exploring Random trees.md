# My thoughts
- It may not give that much of a benefit with few agents compared to round robin as see in [[#7.3.4 Analysis]]
- In this case we are not trying to reach a single goal but a multitude of goals (as given by the slicer)


# Intro
> [!PDF|yellow] [[rrts.pdf#page=1&selection=70,0,73,29&color=yellow|rrts, p.385]]
> > As autonomous systems are deployed in increasingly complex scenarios, it is essential for a team of agents to be able to work in parallel and perform far more complicated tasks than an agent operating alone

> [!PDF|yellow] [[rrts.pdf#page=1&selection=78,46,80,39&color=yellow|rrts, p.385]]
> > agents must also avoid collisions with objects in the environment and other agents while executing these task

> [!PDF|yellow] [[rrts.pdf#page=2&selection=17,15,18,59&color=yellow|rrts, p.386]]
> > is widely recognized that any decentralized planner must be careful to avoid conflicting local decisions

> [!PDF|yellow] [[rrts.pdf#page=2&selection=21,46,23,26&color=yellow|rrts, p.386]]
> > minimizing a global cost function in a decentralized setting is itself a challenging problem


> [!PDF|yellow] [[rrts.pdf#page=2&selection=31,16,34,13&color=yellow|rrts, p.386]]
> > The agents are assumed to have constrained, nonlinear dynamics, as seen in any practical system, and as a result, may be unable to follow paths generated by simple path planners

> [!PDF|yellow] [[rrts.pdf#page=2&selection=39,7,40,18&color=yellow|rrts, p.386]]
> > the typical path planning objective of minimizing a given cost metric,

> [!PDF|yellow] [[rrts.pdf#page=2&selection=41,48,44,56&color=yellow|rrts, p.386]]
> > The approach presented here is not only able to quickly identify paths that satisfy these constraints but also reuses this path information to avoid conflicting decisions between agents



# Related work
## 2.1 Path planning
> [!PDF|yellow] [[rrts.pdf#page=2&selection=55,6,61,49&color=yellow|rrts, p.386]]
> >  techniques such as A and variants thereof (Koenig and Likhachev 2002; Likhachev and Stentz 2008) have been used to find paths through discretized environments. However, these techniques typically do not scale wel


> [!PDF|yellow] [[rrts.pdf#page=2&selection=64,18,69,45&color=yellow|rrts, p.386]]
> > Several optimization-based planners have been developed, including the Mixed Integer Linear Program (MILP) formulation in Richards et al. (2002) and techniques based on Model Predictive Control (MPC) as in Dunbar and Murray (2006). While these methods can typically find minimum cost paths, they also scale poorl

> [!PDF|yellow] [[rrts.pdf#page=2&selection=82,9,88,20&color=yellow|rrts, p.386]]
> >  Random Trees (RRT) (LaValle 1998), have gained popularity in recent years. These algorithms quickly build a set of feasible paths by connecting points sampled from the environment. The sampling process allows these techniques to handle increasingly complex problems, but as a result, they only provide weaker guarantees, such as probabilistic completenes

> [!PDF|yellow] [[rrts.pdf#page=2&selection=99,0,100,13&color=yellow|rrts, p.386]]
> > The work presented in this paper builds upon the CLRRT algorithm

## 2.2 Decentralized planning
> [!PDF|yellow] [[rrts.pdf#page=3&selection=29,0,31,18&color=yellow|rrts, p.387]]
> > Consensus based approaches allow agents to coordinate actions by exchanging information on the state of each member of the team

> [!PDF|yellow] [[rrts.pdf#page=3&selection=38,39,41,13&color=yellow|rrts, p.387]]
> > propose a related cooperation strategy where agents operate in reserved regions of the map and must reach consensus on any changes to these regions
> 
> 

> [!PDF|note] [[rrts.pdf#page=3&selection=41,15,44,26&color=note|rrts, p.387]]
> > This allows for some asynchronous planning but is conservative since any potential intersection between regions triggers a consensus check, even if it does not correspond to a conflict in time

> [!PDF|yellow] [[rrts.pdf#page=3&selection=48,17,51,7&color=yellow|rrts, p.387]]
> > reachability analysis is used to predict potential trajectories of moving objects. In general, these methods are concerned with agents that are not necessarily cooperative

> [!PDF|yellow] [[rrts.pdf#page=3&selection=73,0,85,49&color=yellow|rrts, p.387]]
> > Prioritized planning techniques also avoid conflicts through sequential planning, where the planning order is determined by the priority assigned to each agent

**Important!**
> [!PDF|note] [[rrts.pdf#page=3&selection=86,23,88,3&color=note|rrts, p.387]]
> >  A search over different priority orders can be used to find an ordering that reduces the global cos

> [!PDF|yellow] [[rrts.pdf#page=3&selection=92,8,95,44&color=yellow|rrts, p.387]]
> >  it is also known that in distributed computer systems that require ordered task execution (such as processor coordination and load balancing), having a fixed execution order can limit system performance

## 2.3 Overview
- This algorithm extends the closed-loop RRT algorithm
	- Uses a simulation to grow the number of feasible paths

> [!PDF|red] [[rrts.pdf#page=3&selection=112,36,117,35&color=red|rrts, p.387]]
> > This coordination strategy draws inspiration from the single subsystem update technique in Trodden and Richards (2006) but takes advantage of the underlying CL-RRT planner to quickly find improved paths and produce a dynamic planning order based on each agent’s incentive to replan

> [!PDF|yellow] [[rrts.pdf#page=3&selection=123,0,125,54&color=yellow|rrts, p.387]]
> > Cooperative DMA-RRT algorithm retains the advantages of the DMA-RRT algorithm, while also allowing agents to modify their teammates’ plans in order to select paths

> [!PDF|yellow] [[rrts.pdf#page=4&selection=3,45,5,46&color=yellow|rrts, p.388]]
> > paths generated by both algorithms are guaranteed to satisfy all interagent constraints, such as collision avoidance


# 3 Path planning with complex constraints: CL-RRT
 The algorithm consists of three components: (1) the tree expansion process, (2) the main execution loop, and (3) the lazy check for feasibility.
 
> [!PDF|General 2] [[rrts.pdf#page=4&selection=18,31,21,26&color=yellow|rrts, p.388]]
> >  CL-RRT does not aim to generate optimal paths. Instead, the primary objective is to quickly identify a set of feasible paths that are guaranteed to satisfy all constraints

> [!PDF|general] [[rrts.pdf#page=4&selection=21,32,25,23&color=general|rrts, p.388]]
> > secondary objective is then to select the path from this set with the lowest cost. Other incremental planners could be used in place of CL-RRT as the core of the DMA-RRT algorithm, as long as they guarantee dynamic feasibility

> [!PDF|general] [[rrts.pdf#page=4&selection=35,9,37,11&color=general|rrts, p.388]]
> > Aoude et al. (2010) also demonstrated that the algorithm can be extended to handle other agents moving in the environment


## 3.1 Tree expansion
1) Randomly sampling a point $x_{sample}$ from the set of all feasible configurations $X_{\text{free}}$ 
2) Picks the closest node $N_{near}$ in the tree

> [!PDF|general] [[rrts.pdf#page=4&selection=330,0,341,46&color=general|rrts, p.388]]
> > This path expansion continues until ˆx reaches xsample or becomes infeasible by violating the constraint

> [!PDF|general] [[rrts.pdf#page=4&selection=372,29,375,47&color=general|rrts, p.388]]
> >  Nodes serve as waypoints when executing the path, and each path is uniquely characterized by its waypoints in conjunction with the dynamics and reference law used to grow it between those points

## 3.2 Execution loop

> [!PDF|general] [[rrts.pdf#page=4&selection=377,3,382,4&color=general|rrts, p.388]]
> >  execution loop of the CL-RRT, outlined in Algorithm 2, uses Algorithm 1 to find paths that take the agent to some predefined goal region Xgoal

> [!PDF|general 2] [[rrts.pdf#page=4&selection=388,1,399,30&color=general 2|rrts, p.388]]
> >  Then, during each iteration of the execution loop, the state estimate ˆx(t + Δt) is used to compute additional potential paths using the tree expansion process

> [!PDF|general] [[rrts.pdf#page=5&selection=168,0,172,1&color=general|rrts, p.389]]
> > Once the new potential paths are added to the tree, the minimum-cost path p∗

> [!PDF|general] [[rrts.pdf#page=5&selection=176,27,184,33&color=general|rrts, p.389]]
> > constraints may change over time. To avoid selecting paths that have become infeasible, the lazy check described in Algorithm 3 is performed on p∗ before confirming it as the new path

> [!PDF|general] [[rrts.pdf#page=5&selection=202,0,219,22&color=general|rrts, p.389]]
> > The lazy check mimics the tree expansion process, but instead of generating new nodes, it re-simulates the agent’s trajectory through the nodes defining p∗. If the re-simulated state ˆx(t + k) violates a constraint, the path is pruned beyond the last feasible node

# 4 Decentralized coordinate planning
> [!PDF|yellow] [[rrts.pdf#page=6&selection=11,0,15,30&color=yellow|rrts, p.390]]
> > The approach taken here instead relies on a measure of Potential Path Improvement (PPI) that reflects an agent’s incentive to replan. The PPI is the potential improvement in its own path cost that an agent expects to see if allowed to update its plan next.

> [!PDF|yellow] [[rrts.pdf#page=6&selection=21,10,24,5&color=yellow|rrts, p.390]]
> > Rather than iterating through a list of agents, a token is used to identify which agent is allowed to update its plan at each planning iteration.

> [!PDF|important] [[rrts.pdf#page=6&selection=25,0,27,4&color=important|rrts, p.390]]
> > The idea of token-passing for mutual exclusion and access control is widespread in distributed computer systems

> [!PDF|yellow] [[rrts.pdf#page=6&selection=43,43,50,56&color=yellow|rrts, p.390]]
> > agents without the token compute their PPI and broadcast these values as bids to be the next token holder. When the current token holder is finished replanning, it passes the token to the agent with the best bid, i.e., the greatest PPI. If there is a tie, one of the agents is selected at random. This effectively produces a dynamic planning order where agents that may benefit the most from replanning are able to do so sooner


> [!PDF|general] [[rrts.pdf#page=5&selection=407,0,413,58&color=general|rrts, p.389]]
> > Agents 1 and 2 begin with feasible plans but find alternate plans that appear to be safe given their current knowledge of the other agent. Since this is fully asynchronous, the agents can update their plans at any time. However, if both update at the same time, the new paths are not guaranteed to be safe since the constraints imposed while selecting the paths do not match the constraints that exist when the agents begin


### 4.1.1 PPI from RRT
> [!PDF|yellow] [[rrts.pdf#page=6&selection=63,0,81,0&color=yellow|rrts, p.390]]
> > Computing the PPI requires the agent to compare costs between the current plan and a new plan, i.e., PPI = cost(pcurrent) − cost(pnew)

> [!PDF|yellow] [[rrts.pdf#page=6&selection=91,8,92,58&color=yellow|rrts, p.390]]
> >  the tree of feasible paths maintained in the CLRRT algorithm can be used to compute the PPI very quickly.

> [!PDF|yellow] [[rrts.pdf#page=6&selection=94,7,96,3&color=yellow|rrts, p.390]]
> > the current token-holder) does not need to compute its PPI. So it simply selects the best path in its tree (as in CLRRT

> [!PDF|important] [[rrts.pdf#page=6&selection=96,35,101,22&color=important|rrts, p.390]]
> > gents without the token continue executing their previously selected plans, but also continue to grow their own trees in order to compute their PPI. By continuing to grow their trees, it is easy for agents to identify new, lower-cost paths by simply searching the leaf nodes of the tree

> [!PDF|yellow] [[rrts.pdf#page=6&selection=138,0,141,22&color=yellow|rrts, p.390]]
> > For example, if the best path in the tree has a much lower cost than the path the agent is currently taking, the PPI would be large and it would be beneficial to replan soon to select the better path



# 5 Decentralized multi-agent RRT
## Assumptions
> [!PDF|general] [[rrts.pdf#page=7&selection=9,27,19,42&color=general|rrts, p.391]]
> > The individual component handles the path planning, while the interaction component handles all information received from other agents

> [!PDF|general] [[rrts.pdf#page=7&selection=25,0,28,8&color=general|rrts, p.391]]
> > Network: The agents are assumed to form a fully connected network, allowing for fast information transfer across the team

> [!PDF|general] [[rrts.pdf#page=7&selection=41,0,43,47&color=general|rrts, p.391]]
> > Agent models: Each agent is assumed to have a model of its own dynamics, as in the CL-RRT algorithm

- The obstacles do not change significantly during a single layer increment so it might be safe to assume this is satisfied
> [!PDF|red] [[rrts.pdf#page=7&selection=52,0,54,60&color=red|rrts, p.391]]
> > Environment: The environment that the agents operate in is assumed to be known and only contain static obstacles.
> 
> 

> [!PDF|general] [[rrts.pdf#page=7&selection=61,1,66,17&color=general|rrts, p.391]]
> > Inter-agent constraints: Finally, the set of constraints imposed between agents, such as collision avoidance or rendezvous requirements, are assumed to be symmetric between agent pairs

## 5.2 Individual Component
> [!PDF|yellow] [[rrts.pdf#page=7&color=yellow|rrts, p.391]]
> > 5.2 Individual component
> 
> 
1) Agent is initialized with a feasible path
2) One agent is chosen to be token holder
3) Each agent grows tree of feasible paths and at the end of planning picks the best path $p_{k}^{*}$
4) Merit strategy determines if the agent will update its path to $p_{k}^{*}$ and pass the token to someone else or if it will bit **<-- i don't really understand this yet**
5) If agent updates plan, it broadcasts the waypoints to other agents
## 5.3 Interaction Component

> [!PDF|yellow] [[rrts.pdf#page=7&selection=358,0,374,42&color=yellow|rrts, p.391]]
> > Communication between agents involves two different message types. The first has a list of waypoints corresponding to an updated plan and the name of the token winner, and it is sent after the current token holder updates its plan. The second message is just the PPI bid and is sent when an agent without the token finds a better path

> [!PDF|yellow] [[rrts.pdf#page=8&selection=112,0,125,11&color=yellow|rrts, p.392]]
> > This trajectory information can then be used in the CLRRT’s feasibility checks to ensure the path satisfies all interagent constraints. When the token winner receives the waypoints and winner message, it becomes the token holder (line 6) and can select a new plan as soon as it updates its constraints

# 6 Cooperative DMA-RRT
> [!PDF|yellow] [[rrts.pdf#page=9&selection=356,0,360,10&color=yellow|rrts, p.393]]
> > Although the DMA-RRT algorithm implements a coordination strategy, each agent only aims to minimize its own path cost when selecting a path from the tree. However, this locally greedy approach does not necessarily minimize the global cos

> [!PDF|yellow] [[rrts.pdf#page=9&selection=365,7,369,36&color=yellow|rrts, p.393]]
> > or example, consider a cluttered environment with narrow passages between obstacles. It is then possible for agents to select paths that are conflict-free but obstruct these passages, thus preventing multiple agents from making additional progress toward their goals

> [!PDF|yellow] [[rrts.pdf#page=9&selection=383,0,386,23&color=yellow|rrts, p.393]]
> > This section introduces a cooperation strategy that allows an agent to modify its own path as well as the path of another agent in order to minimize the combined path cost and thus reduce the global cost.

> [!PDF|yellow] [[rrts.pdf#page=9&selection=391,36,401,19&color=yellow|rrts, p.393]]
> > he resulting Cooperative DMA-RRT algorithm introduces emergency stop nodes along each agent’s path where the agent could safely terminate the path if asked to by another agent. The decision to terminate another agent’s path is based on a cost comparison between teammates and enables the team to avoid costly deadlock scenarios.

# 7 Simulation
## 7.3.4 Analysis
> [!PDF|yellow] [[rrts.pdf#page=15&selection=69,0,76,32&color=yellow|rrts, p.399]]
> > For a team of only four agents, the merit-based token passing strategy is not expected to provide as significant an improvement in performance, in general (except in cases where, for example, one agent has a much more challenging route to plan). This can be seen from a comparison of Fig. 8(a) and Fig. 8(b). Even with the dynamic planning order, there are so few agents that each receives the token almost as regularly as in the round-robin case

