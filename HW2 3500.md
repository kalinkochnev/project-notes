# Problem 0
1) $T(n) = 2T(\frac{n}{2})+O(n)$
$O(n\log n)$. At each level the amount of work done is $2^{k} O(\frac{n}{2^{k}})=O(n)$ since there are $2^{k}$ nodes per level $k$ and each node does $O(\frac{n}{2^{k}})$ work (since the amount of work gets halved at every level). So the work is evenly distributed throughout the tree. Since there are $\log_{2}(n)$ levels, the total amount of work is $\text{\# of levels} * \text{work per level}$. The asymptotic bound for $T(n)=O(n\log n)$.

2) $T(n)=2T(\frac{n}{2})+O(1)$
$O(n)$. At each level there are $2^{k}$ nodes and each node does $O(1)$ work, so $2^{k}O(1)$ work per level. The larger $k$ gets, the more work there is at that level. So $T(n)$ is bottom heavy. 

Since the amount of work per level varies with the depth, we must use a sum.
$$
\begin{align}
\text{total work = }\sum_{k=0}^{\log_{2}n}2^{k}O(1)=O(1) \sum_{k=0}^{\log_{2}n}2^{k}
\end{align}
$$
This is a geometric series sum so
$$
\begin{align}
\text{total work} = O(1)\left( \frac{1-2 ^{\log_{2}n}}{1-2} \right) \\
=O(1)\left( \frac{1-n}{-1} \right) \\
=O(n)-O(1) \\
=O(n)
\end{align}
$$

3) $T(n)=7T(\frac{n}{2})+O(n^{3})$
$O(n^{2+\log_{2}(7)})$. There are $7^{k}$ nodes per level and $O(\frac{n^{3}}{2^{k}})$ work per node (since work is halved at every level), so $O\left( \left( \frac{7}{2} \right)^{k} n^{3} \right)$ work per level. Since $\frac{7}{2}>1$, this indicates that the amount of work per level increases with the depth. So $T(n)$ is bottom heavy.

Since the amount of work depends on the depth, we must use a sum.

$$
\begin{align}
\text{total work} = \sum_{k=0}^{\log_{2}n } O\left( \frac{7}{2}^{k} \right)O(n^{3}) \\
=O(n^{3})\left( \frac{1-\left( \frac{7}{2}^{\log_{2}n} \right)}{1-\frac{7}{2}} \right) \\ 
=O(n^{3})\left( 0.4\left( \frac{7^{\log_{2}n}}{2^{\log_{2}n}} \right)-0.4\right) \\
=O(n^{3})\left( 0.4\left( \frac{2^{\log_{2}(7)\log_{2}(n)}}{n} \right)-0.4 \right) \\
=O(0.4n^{2}n^{\log_{2}(7)}-0.4n^{3}) \\
=O(n^{2+\log_{2}(7)})
\end{align}
$$

4) $T(n)=7T(\frac{n}{2})+O(n^{2})$
$O(n^{1+\log_{2}(7)})$. At each level there are $7^{k}$ nodes and each node does $O(\frac{n^{2}}{2^{k}})$ work. The total work per level is $O(\frac{7}{2}^{k}n^{2})$. The recurrence is bottom-heavy because as the depth increases, $O(\frac{7}{2}^{k}n^{2})$ increases.

$$
\begin{align}
\text{total work} = \sum_{k=0}^{\log_{2}n } O\left( \frac{7}{2}^{k} \right)O(n^{3}) \\
=O(n^{2})\left( \frac{1-\left( \frac{7}{2}^{\log_{2}n} \right)}{1-\frac{7}{2}} \right) \\ 
=O(n^{2})\left( 0.4\left( \frac{7^{\log_{2}n}}{2^{\log_{2}n}} \right)-0.4\right) \\
=O(n^{2})\left( 0.4\left( \frac{2^{\log_{2}(7)\log_{2}(n)}}{n} \right)-0.4 \right) \\
=O(0.4n^{1}n^{\log_{2}(7)}-0.4n^{2}) \\
=O(n^{1+\log_{2}(7)})
\end{align}
$$

5) $T(n)=4T(\frac{n}{2})+O(n^{2}\sqrt{ n })$
$O(n^{2.5})$. At each level there are $4^{k}$ nodes and each node does $O(\left( \frac{n}{2^{k}}\right)^{2}\sqrt{ \frac{n}{2^{k}} })$ work. The work per level is 
$$
\begin{align}
O\left( 4^{k}\left( \frac{n}{2^{k}}\right)^{2}\sqrt{ \frac{n}{2^{k}} } \right)=O\left( \frac{4^{k}}{(2^{2})^{k}} \frac{1}{(2^{1/2})^{k}} n^{2}n^{1/2}\right) \\
=O\left( \frac{n^{2.5}}{\sqrt{ 2 }^{k}} \right)
\end{align}
$$
The recurrence is top heavy because as the $k$ increases, the amount of work per level decreases (since $\sqrt{ 2 }^{k}$ gets larger in the denominator).

Since the amount of work changes based on the level, we must use a sum.
$$
\begin{align}
\sum_{k=0}^{\log_{2}n} O\left( \frac{1}{\sqrt{ 2 }^{k}} \right)O(n^{2.5})=O(n^{2.5})\left( \frac{1-2^{-0.5\log_{2}(n)}}{1-2^{-0.5}} \right)  \\
=O(n^{2.5})\left( \frac{1-n^{-0.5}}{1-2^{-0.5}} \right)  
=O(n^{2.5})(3.41 - 3.41n^{-0.5}) \\
=O(n^{2.5}-n^{2})=O(n^{2.5})
\end{align}
$$

6) $T(n)=4T(\frac{n}{2})+O(n\log_{2}n)$
At each level there are $4^{k}$ nodes and each node does $O(\frac{n}{2^{k}}\log_{2}\left( \frac{n}{2^{k}} \right))$ work. The work per level is $O(2^{k}n\log_{2}\left( \frac{n}{2^{k}} \right))=O(2^{k}n\left[\log_{2}(n)-\log_{2}(2^{k})\right])=O(2^{k}n\left[\log_{2}(n)-k\right])$. As $k$ increases, . Therefore $T(n)$ is bottom heavy.

$$
\begin{align}
\text{total work} = \sum_{k=0}^{\log_{2}n} O(2^{k}n\log_{2}(n)-k 2^{k}n) \\
=O(n) \sum_{k=0}^{\log_{2}n} O(2^{k}\log_{2}(n)-k 2^{k}) \\
=O(n) \sum_{k=0}^{\log_{2}n} O(2^{k})O(\log_{2}(n)-k)
\end{align}
$$

# Problem 1
We will show it is possible to tile any $2^{k}\times 2^{k}$ board with $k\geq 1$ and the upper left hand corner removed using induction. We will show how to divide the problem into sub-problems and how to re-combine them in order to get complete solution.

**Base case: k=1**
The solution for a $2^{1} \times 2^{1}$ board is trivial since a single L-shape will cover the board. Let the "x" symbol represent the removed square and the "o" symbol represent a part of the L piece.

x o
o o

**Inductive step:**
Assume the inductive hypothesis where we can tile a $2^{k-1} \times 2^{k-1}$ board using L-shaped pieces. We will show how to extend such a solution to a $2^{k} \times 2^{k}$ board. 

If we have an a $2^{k} \times 2^{k}$ board we can start by dividing it into four quadrants. So a single quadrant is $2^{k} \times 2^{k} / 4 = \frac{1}{2}2^{k} \times \frac{1}{2}2^{k}=2^{k-1} \times 2^{k-1}$
![[lshapechess.png]]
With each quadrant, a corner piece is missing if we allow the center piece to be our trivial case of a single L piece. By the inductive hypothesis, we know how to tile each $2^{k-1} \times 2^{k-1}$ board using L-shaped pieces assuming a single corner is missing.

# Problem 2
The following algorithm is a modification to merge sort, specifically the merge algorithm. We will show that the asymptotic time complexity of the modified merge algorithm is the same as that of the original $O(n)$. We will assume that the rest of merge sort is the same so that the overall complexity is $O(n\log n)$.

The base case will be arrays of size 1. The invariant that will be made use of during the merge algorithm is that the two input arrays $A$ and $B$ are both sorted and the combined array $A \cup B$ is also sorted. 

The modified merge algorithm is as follows (in psuedocode)
```python
def merge(A: list, B: list, n_inverted):
	# n_inverted is a variable that counts the total number of inverted pairs
	merged = [] # final merged array
	i, j = 0 # i indexes A and j indexes B

	# While there are elements that have not been merged in
	while (i+1 < len(A)) and (j+1 < len(B)):
		if A[i] > B[j]:
			n_inverted += len(A) - i # NOTE (1)
			merged.append(B[j])
			j += 1
		else: # there is not an inverted pair for A's element
			merged.append(A[i])
			i += 1
	
```

For instance, let $A=a_{0},a_{1} \dots a_{m}$ and $B=b_{0},b_{1}\dots b_{n}$. Let $i$ be the current index of an element in $A$ so $0\leq i\leq m$ and $j$ be the current index of an element in $B$ so $0\leq j\leq n$. 

The code is functionally the same as the standard merge. We check if $A[i] > B[j]$. If $A[i]$ is greater, we add $B[j]$ onto the merged array and increment $j$ so the next iteration with compare the next element of B. Likewise, if $A[i]$ is smaller, we add it to the merged array. This results in elements being added onto `merged` in ascending order. There are at most `len(A) + len(B)` iterations (since we iterate through every element in A and B once). In the body of the loop, including the modification, there are only $O(1)$ operations. So the total runtime of the modified merge is $O(n)$ as needed.

The main change occurs where where it says **NOTE (1)** in the comments. If it is the case that $A[i] > B[j]$, we know that $(A[i], B[j])$ is an inverted pair. 
		- The order of the initial input $L$ affects how many inverted pairs there are. However, when we partition the array and go to merge, we know that everything on the left array $A$ had a lower index than everything on the right array $B$. So even thought A and B are now sorted, we know that everything in $A$ has lower indexes than everything in $B$.
		- Additionally, $A[i]>B[j]$ as we checked.
The additional property that allows us to maintain linear time while still counting all inverted pairs is as follows. Both arrays are sorted so we know $A[m] \geq A[m-1]\geq\dots\geq A[i] \geq B[j]$. Therefore, every element $[i,m]$ in $A$ is also an inverted pair since. `n_inverted += len(A) - i` calculates that interval and increases the total number of inverted pairs.

The `else` case does not affect the number of inverted pairs because $A[i] < B[j]$ which does not satisfy the definition of an inverted pair.

The runtime 

# Problem 3
### 3.1) Solve the BSM problem.
The algorithm has two parts. The first half attempts to find the index $i$ that maximizes the sum between $(i,m)$ where $m$ is the middle element's index. It iterates through the range $[0, m)$ while keeping track of the total. If the addition of a particular index raises the total, we make note of the new best total and the index it occurred at. A similar process occurs for the right side except iterating through the range $(m, len(L))$ where $L$ is the input array.

```python
def best_subset_middle(L: list):
	middle = len(L) //2 
	i, j = middle, middle # i and j both initialized to half list size
	best_i, best_j = middle, middle
	best_left, best_right = 0, 0 # track sums of left and right sise

	# First find the index i that maximizes the sum of elements from [0, middle)
	total = 0
	for i in range(0, middle):
		elem = L[middle - i]
		total += elem
		
		if total > best_left:
			best_i = middle - i
			best_left = total

	# Find the index j that maximizes the sum of thelements between (middle, len(L))
	total = 0
	for j in range(middle+1, len(L)):
		elem = L[j]
		total += elem
		if total > best_right:
			best_j = j
			best_right = total

	sum_interval = best_left + best_right + L[middle]
	return (best_i, best_j, sum_interval)				
```

When we iterated, we did not include the middle element as a part of the total. Our final total for the subset is the sum of best left total, the best right total, and the middle element. Each loop only iterates through $\frac{n}{2}-1$ elements in the array. There are two loops and the body of each only has constant time operations. So the total runtime of the whole algorithm is $O(n)$.

### 3.2) Solve the best subset problem using a recursive algorithm
Aside from the helper code, we start by splitting the 
There are a two main cases with a few sub-cases that depend on the output of `best_subset_middle`. Let $(A_{i}, A_{j})$ represent the BSM interval of $A$, and likewise for $B$ $(B_{i}, B_{j})$. 

```python
def best_subset(A, start, end):
	n = len(A)
	if n == 1:
		return (0, 0)

	(left_i, left_j) = best_subset(A, 0, n//2)
	(right_i, right_j) = best_subset(A, n//2 + 1, n-1)

	# Compute the best subset middle of the interval (left_i, right_j)
	(i, j) = best_subset_middle(A, left_i, right_j)
	return (i, j)
```

**Case 1**:  $A_{j} = len(A)-1$ and $B_{i} = len(B)-1$ 
This implies that the union of $(A_{i}, A_{j}) \cup \text{middle } \cup (B_{i}, B_{j})$ would be one contiguous interval. If that is the case


### 3.4) Best subset $O(n)$
```python
def recr_best_subset(L: list, i, j):
	(i_new, j_new, total) = best_subset_middle(L, i, j)
	if (i_new, j_new) == (i, j):
		return (i, j)
	return recr_best_subset(L, i_new, j_new)
```

